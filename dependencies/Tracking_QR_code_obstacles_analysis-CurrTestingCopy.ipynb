{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade opencv-python\n",
    "#!pip install opencv-python numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from shapely.geometry import LineString\n",
    "from shapely.geometry import Point\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "from shapely.geometry import LineString\n",
    "import colorsys\n",
    "import pyvisgraph as vg\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_Green_square(image, min_blue, min_green, min_red, max_blue, max_green, max_red, kernel_size=5):\n",
    "    \n",
    "    # Taking a matrix of size 5 as the kernel \n",
    "    kernel = np.ones((5, 5), np.uint8) \n",
    "    \n",
    "     # HSV (Hue, Saturation, Value): Separates the color information from the brightness information, making it robust to changes in lighting conditions\n",
    "    hsv_frame = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    #getting the mask image from the HSV image using threshold values\n",
    "    mask = cv2.inRange(hsv_frame, (min_blue, min_green, min_red), (max_blue, max_green, max_red))\n",
    "    mask_dilation = cv2.dilate(mask, kernel, iterations=1)\n",
    "    mask_erosion = cv2.erode(mask_dilation, kernel, iterations=1) \n",
    "    \n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(mask_erosion)\n",
    "    plt.title(\"mask green\")\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n",
    "    \n",
    "    return mask_erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, min_blue, min_green, min_red, max_blue, max_green, max_red, kernel_size=5):\n",
    "    lower_threshold = 100\n",
    "    upper_threshold = 150\n",
    "    aperture_size = 7\n",
    "    kernel = np.ones((5, 5), np.uint8) \n",
    "    \n",
    "    # HSV (Hue, Saturation, Value): Separates the color information from the brightness information, making it robust to changes in lighting conditions\n",
    "    hsv_frame = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    #getting the mask image from the HSV image using threshold values\n",
    "    mask = cv2.inRange(hsv_frame, (min_blue, min_green, min_red), (max_blue, max_green, max_red))\n",
    "#     #Display the image using matplotlib\n",
    "#     plt.imshow(mask)\n",
    "#     plt.title(\"mask\")\n",
    "#     plt.axis('off')  # Turn off axis labels\n",
    "#     plt.show()\n",
    "    mask_dilation = cv2.dilate(mask, kernel, iterations=1)\n",
    "    mask_erosion = cv2.erode(mask_dilation, kernel, iterations=1) \n",
    "    inverted_image = cv2.bitwise_not(mask_erosion)\n",
    "    med_img   = cv2.medianBlur(inverted_image,kernel_size)\n",
    "    canny_img = cv2.Canny(med_img, lower_threshold, upper_threshold, apertureSize=aperture_size, L2gradient=True)\n",
    "    dilated_edges = cv2.dilate(canny_img, kernel, iterations=1)\n",
    "\n",
    "#     #Display the image using matplotlib\n",
    "#     plt.imshow(dilated_edges)\n",
    "#     plt.title(\"mask\")\n",
    "#     plt.axis('off')  # Turn off axis labels\n",
    "#     plt.show()\n",
    "    \n",
    "    return dilated_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting image to top view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/perspective-transformation-python-opencv/\n",
    "# https://note.nkmk.me/en/python-opencv-qrcode/\n",
    "# https://docs.opencv.org/4.x/dd/d49/tutorial_py_contour_features.html  \n",
    "# https://theailearner.com/tag/cv2-minarearect/\n",
    "\n",
    "def perspective_transformation(image):\n",
    "        \n",
    "    # Destination points for the matrix transformation\n",
    "    #dest_corners =np.float32([(width, height), (0, height), (width, 0), (0, 0)])\n",
    "    height, width, _ = image.shape\n",
    "    dest_corners = np.float32([(0, height), (width, height), (0, 0), (width, 0)])\n",
    "    \n",
    "    # Initialize a list to store the centers of the detected objects\n",
    "    centers = []\n",
    "    \n",
    "    # Mask values of the object to be detected\n",
    "    (min_blue, min_green, min_red) = (44, 52, 73)\n",
    "    (max_blue, max_green, max_red) = (103, 255, 146)\n",
    "\n",
    "    processed_mask = process_Green_square(image, min_blue, min_green, min_red, max_blue, max_green, max_red)\n",
    "    \n",
    "    #extracting the contours of the object\n",
    "    contours,_ = cv2.findContours(processed_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    #sorting the contour based of area\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    # Take the top 4 contours\n",
    "    top_contours = contours[:4]\n",
    "\n",
    "    #print('number of contours', len(top_contours))\n",
    "\n",
    "    # Extract the 4 biggest contours wich are not having the same center\n",
    "    for contour in top_contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        center = (x + w // 2, y + h // 2)\n",
    "\n",
    "        # Check if the center is not close to any existing centers\n",
    "        if all(np.linalg.norm(np.array(center) - np.array(existing_center)) > 50 for existing_center in centers):\n",
    "            centers.append(center)\n",
    "            cv2.rectangle(image, (x - 15, y - 15), (x + w + 15, y + h + 15), (0, 255, 0), 4)\n",
    "    \n",
    "    if len(centers) == 4:\n",
    "        center_points = np.float32(centers).reshape(-1, 1, 2)\n",
    "        transformation_matrix = cv2.getPerspectiveTransform(center_points, dest_corners)\n",
    "        return transformation_matrix\n",
    "    else:\n",
    "    # Return the initial image if not enough contours are detected\n",
    "        transformation_matrix = None\n",
    "        return transformation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to adjsut threshold (not in the final submition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty function\n",
    "def doNothing(x):\n",
    "    pass\n",
    "\n",
    "def find_thresh(image):\n",
    "    #creating a resizable window named Track Bars\n",
    "    cv2.namedWindow('Track Bars', cv2.WINDOW_NORMAL)\n",
    "\n",
    "    #creating track bars for gathering threshold values of red green and blue\n",
    "    cv2.createTrackbar('min_blue', 'Track Bars', 0, 255, doNothing)\n",
    "    cv2.createTrackbar('min_green', 'Track Bars', 0, 255, doNothing)\n",
    "    cv2.createTrackbar('min_red', 'Track Bars', 0, 255, doNothing)\n",
    "\n",
    "    cv2.createTrackbar('max_blue', 'Track Bars', 0, 255, doNothing)\n",
    "    cv2.createTrackbar('max_green', 'Track Bars', 0, 255, doNothing)\n",
    "    cv2.createTrackbar('max_red', 'Track Bars', 0, 255, doNothing)\n",
    "\n",
    "    resized_image = cv2.resize(image,(800, 626))\n",
    "    #converting into HSV color model\n",
    "    hsv_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    #showing both resized and hsv image in named windows\n",
    "    #cv2.imshow('Base Image', resized_image)\n",
    "    #cv2.imshow('HSV Image', hsv_image)\n",
    "\n",
    "\n",
    "    #creating a loop to get the feedback of the changes in trackbars\n",
    "    while True:\n",
    "        #reading the trackbar values for thresholds\n",
    "        min_blue = cv2.getTrackbarPos('min_blue', 'Track Bars')\n",
    "        min_green = cv2.getTrackbarPos('min_green', 'Track Bars')\n",
    "        min_red = cv2.getTrackbarPos('min_red', 'Track Bars')\n",
    "\n",
    "        max_blue = cv2.getTrackbarPos('max_blue', 'Track Bars')\n",
    "        max_green = cv2.getTrackbarPos('max_green', 'Track Bars')\n",
    "        max_red = cv2.getTrackbarPos('max_red', 'Track Bars')\n",
    "\n",
    "        #using inrange function to turn on the image pixels where object threshold is matched\n",
    "        mask = cv2.inRange(hsv_image, (min_blue, min_green, min_red), (max_blue, max_green, max_red))\n",
    "        #showing the mask image\n",
    "        cv2.imshow('Mask Image', mask)\n",
    "        # checking if q key is pressed to break out of loop\n",
    "        key = cv2.waitKey(25)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    #printing the threshold values for usage in detection application\n",
    "    print(f'min_blue {min_blue}  min_green {min_green} min_red {min_red}')\n",
    "    print(f'max_blue {max_blue}  max_green {max_green} max_red {max_red}')\n",
    "    #destroying all windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing each vertex in different colors (not used in the final submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vertex_circles(image, points):\n",
    "    # Ensure there is at least one set of vertices\n",
    "    if len(points) < 1 or len(points[0]) < 3:\n",
    "        raise ValueError(\"The function expects at least one set of three vertices.\")\n",
    "\n",
    "    # Extract vertices from the array\n",
    "    vertices_np = np.array(points[0], dtype=np.int32)\n",
    "\n",
    "    # Define colors for each vertex\n",
    "    colors = [\n",
    "        (0, 0, 255),  # Red for the first vertex\n",
    "        (0, 255, 0),  # Green for the second vertex\n",
    "        (255, 0, 0),  # Blue for the third vertex\n",
    "        (255, 255, 0),  # Yellow for the fourth vertex (and so on...)\n",
    "    ]  # BGR format\n",
    "\n",
    "    # Draw circles around each vertex with different colors\n",
    "    for i, vertex in enumerate(vertices_np):\n",
    "        color = colors[i % len(colors)]  # Cycle through colors if there are more vertices\n",
    "        cv2.circle(image, tuple(vertex), 5, color, -1)  # -1 fills the circle\n",
    "\n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title(\"Image with Vertex Circles and Polyline\")\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angle of the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation_angle(points):\n",
    "    \n",
    "    points_np = np.array(points[0], dtype=np.float32)\n",
    "\n",
    "    # Calculate the centroid (center) of the robot\n",
    "    robot_center = np.mean(points_np, axis=0)\n",
    "\n",
    "    # Choose one vertex as a reference (e.g., the first vertex)\n",
    "    right_front = points_np[0]\n",
    "    left_front = points_np[3]\n",
    "    center_front = ((right_front[0] + left_front[0]) / 2, (right_front[1] + left_front[1]) / 2)\n",
    "\n",
    "    # Calculate the vector from the centroid to the reference vertex\n",
    "    vector_to_reference = center_front - robot_center\n",
    "\n",
    "    # Calculate the orientation angle in degrees in the range of -180 to 180 degrees\n",
    "    angle = (np.arctan2(vector_to_reference[1], vector_to_reference[0]) * 180 / np.pi + 180) % 360 - 180\n",
    "\n",
    "    return angle, robot_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean angle over a window of frames\n",
    "def calculate_mean_angle(angle_list):\n",
    "    return sum(angle_list) / len(angle_list) if len(angle_list) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the contours found to detect the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectShape(cnt):          #Function to determine type of polygon on basis of number of sides\n",
    "    shape = 'unknown' \n",
    "    peri=cv2.arcLength(cnt,True) \n",
    "    vertices = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
    "    sides = len(vertices)\n",
    "    #print('sides', sides)\n",
    "    if (sides == 3): \n",
    "        shape='triangle' \n",
    "    elif (sides == 5):\n",
    "        shape='pentagon'\n",
    "    elif(sides==8): \n",
    "        shape='octagon' \n",
    "    else:\n",
    "        shape='circle' \n",
    "    return shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS: a contour, and minimum distance need to scale the contour\n",
    "def scale_contour(original_contour, desired_min_distance):\n",
    "    # Get the bounding rectangle around the shape\n",
    "    x, y, w, h = cv2.boundingRect(original_contour)\n",
    "\n",
    "    # Calculate the center of the bounding rectangle\n",
    "    center = ((x + w // 2), (y + h // 2))\n",
    "    \n",
    "    scaled_adequate = False\n",
    "    scale_factor = 1.3;\n",
    "    \n",
    "    while (not scaled_adequate):\n",
    "        # Scale each point of the contour relative to the center\n",
    "        scaled_contour = np.array([[(point[0][0] - center[0]) * scale_factor + center[0],\n",
    "                                (point[0][1] - center[1]) * scale_factor + center[1]]\n",
    "                               for point in original_contour], dtype=np.int32)\n",
    "        #print(scaled_contour)\n",
    "        # checking if contour is scaled enough\n",
    "        min_distance = float('inf')\n",
    "\n",
    "        #print(original_contour)\n",
    "        for point in scaled_contour:\n",
    "            point = tuple(float(coord) for coord in point)\n",
    "            distance = cv2.pointPolygonTest(original_contour, point, True)\n",
    "            min_distance = min(min_distance, abs(distance))\n",
    "        #print(min_distance)\n",
    "        if (min_distance < desired_min_distance):\n",
    "            scale_factor += 0.01\n",
    "#             print(scale_factor)\n",
    "        else:\n",
    "            scaled_adequate = True\n",
    "            print(\"adequate\")\n",
    "    \n",
    "    return scaled_contour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the obstacles in the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_obstacles(contours):\n",
    "    obstacle_vertices = []  # List to store vertices for each triangle\n",
    "    obstacle_edges = []  # List to store lines for each triangle\n",
    "    num_obstacles = 0\n",
    "\n",
    "    for cnt in contours:\n",
    "        #shape = detectShape(cnt)\n",
    "        peri=cv2.arcLength(cnt,True)\n",
    "        vertices = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
    "        sides = len(vertices)\n",
    "        print(sides)\n",
    "        \n",
    "        if sides == 4:\n",
    "            num_obstacles += 1\n",
    "            #print('shape',shape)\n",
    "            minimum_distance = 10\n",
    "            cnt = scale_contour(cnt, minimum_distance)\n",
    "\n",
    "            vertices = cv2.approxPolyDP(cnt, 0.02 * cv2.arcLength(cnt, True), True)\n",
    "            obstacle = []  # Store vertices for each obstacle\n",
    "            edges = []  # Store lines for each obstacle\n",
    "            \n",
    "            for i, vertex in enumerate(vertices):\n",
    "                x, y = vertex[0]\n",
    "                obstacle.append((x, y))\n",
    "\n",
    "                # Calculate the index of the next vertex in the list (wrapping around to the first vertex if it's the last one)\n",
    "                next_index = 0 if i == len(vertices) - 1 else i + 1\n",
    "                next_vertex = vertices[next_index][0]\n",
    "\n",
    "                # Append the current edge to the list of edges\n",
    "                edges.append(((x, y), (next_vertex[0], next_vertex[1])))\n",
    "\n",
    "            obstacle_vertices.append(obstacle)  # Append the vertices to the list\n",
    "            obstacle_edges.append(edges)  # Append the edges to the list\n",
    "    return obstacle_vertices, obstacle_edges, num_obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying the Goal in the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_goal(contours):\n",
    "\n",
    "    goal_center = None\n",
    "    \n",
    "    for cnt in contours:\n",
    "        shape = detectShape(cnt)\n",
    "        #print('shape',shape)\n",
    "        if shape == 'octagon':\n",
    "            # Store circle information\n",
    "            (goal_center, radius) = cv2.minEnclosingCircle(cnt)\n",
    "            goal_center = (int(goal_center[0]), int(goal_center[1]))\n",
    "            radius = int(radius)\n",
    "\n",
    "    return goal_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if direct lines are possible between vertices taking in account the obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_intersection(vertex1, vertex2, triangle1_edges, triangle2_edges):\n",
    "    line = LineString([vertex1, vertex2])\n",
    "    #print('line1', line)\n",
    "    for edge in triangle1_edges:\n",
    "        for i in range(len(edge)):\n",
    "            if i != len(edge) - 1:\n",
    "                edge1 = LineString([edge[i], edge[i + 1]])\n",
    "            else:\n",
    "                edge1 = LineString([edge[i], edge[0]])\n",
    "\n",
    "            if (line.coords[0] in edge1.coords) or (line.coords[1] in edge1.coords):\n",
    "                #print('The line shares a common point with triangle 1 edge:', edge1)\n",
    "                break  # Disregard connection if the same vertex is part of both lines\n",
    "\n",
    "            elif line.intersects(edge1):\n",
    "                #print('Intersection found with triangle 1 edge:', edge1)\n",
    "                return True  # If an intersection is detected, return True\n",
    "\n",
    "    for edge in triangle2_edges:\n",
    "        for i in range(len(edge)):\n",
    "            if i != len(edge) - 1:\n",
    "                edge2 = LineString([edge[i], edge[i + 1]])\n",
    "            else:\n",
    "                edge2 = LineString([edge[i], edge[0]])\n",
    "\n",
    "            if (line.coords[0] in edge2.coords) or (line.coords[1] in edge2.coords):\n",
    "                #print('The line shares a common point with triangle 2 edge:', edge2)\n",
    "                break  # Disregard connection if the same vertex is part of both lines\n",
    "\n",
    "            elif line.intersects(edge2):\n",
    "                #print('Intersection found with triangle 2 edge:', edge2)\n",
    "                return True  # If an intersection is detected, return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating adjancy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(triangle_vertices, triangle_edges, Xcent, Ycent, Xrob_center, Yrob_center):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for i, triangle1 in enumerate(triangle_vertices):\n",
    "        for j, triangle2 in enumerate(triangle_vertices):\n",
    "            if i != j:  # Ensure you're comparing vertices from different triangles\n",
    "                for vertex1 in triangle1:\n",
    "                    for vertex2 in triangle2:\n",
    "                        if vertex1 != vertex2:\n",
    "                            intersection = check_intersection(vertex1, vertex2, triangle_edges[i], triangle_edges[j])\n",
    "                            if not intersection:  # If no intersection, add to the edge if intersection = false add\n",
    "                                G.add_edge(vertex1, vertex2)  # Add edge to the graph\n",
    "                            # Handling the initial position and the goal of the robot\n",
    "                            circle_intersect_v1 = check_intersection(vertex1, (Xcent, Ycent), triangle_edges[i], triangle_edges[j])\n",
    "                            circle_intersect_v2 = check_intersection(vertex2, (Xcent, Ycent), triangle_edges[i], triangle_edges[j])\n",
    "                            robot_intersect_v1 = check_intersection(vertex1, (Xrob_center, Yrob_center), triangle_edges[i], triangle_edges[j])\n",
    "                            robot_intersect_v2 = check_intersection(vertex2, (Xrob_center, Yrob_center), triangle_edges[i], triangle_edges[j])\n",
    "\n",
    "                            if not circle_intersect_v1:\n",
    "                                G.add_edge(vertex1, (Xcent, Ycent))\n",
    "                            if not circle_intersect_v2:\n",
    "                                G.add_edge(vertex2, (Xcent, Ycent))\n",
    "                            if not robot_intersect_v1:\n",
    "                                G.add_edge(vertex1, (Xrob_center, Yrob_center))\n",
    "                            if not robot_intersect_v2:\n",
    "                                G.add_edge(vertex2, (Xrob_center, Yrob_center))\n",
    "\n",
    "    # Iterate through each triangle\n",
    "    for i, triangle_edge in enumerate(triangle_edges):\n",
    "        for edge in triangle_edge:\n",
    "            G.add_edge(edge[0], edge[1])  # Add each edge to the graph\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria for shortest path is the distance between vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_euclidean_distances(G):\n",
    "    euclidean_distances = {}\n",
    "    \n",
    "    for u, v in G.edges:\n",
    "        # Extract coordinates of the vertices (u and v)\n",
    "        x1, y1 = u\n",
    "        x2, y2 = v\n",
    "\n",
    "        # Calculate Euclidean distance between the vertices\n",
    "        distance = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n",
    "\n",
    "        # Store the calculated Euclidean distance in the dictionary\n",
    "        euclidean_distances[(u, v)] = distance\n",
    "\n",
    "    return euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process global obstacles and Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_background(image):\n",
    "    \n",
    "    # Scale_factor\n",
    "    \n",
    "    # Defining the the RGB threshold values for the obstacles\n",
    "    (min_blue_obst, min_green_obst, min_red_obst) = (0, 0, 0)\n",
    "    (max_blue_obst, max_green_obst, max_red_obst) = (255, 199, 44)\n",
    "    \n",
    "    # Defining the the RGB threshold values for the goal destination\n",
    "    (min_blue_goal, min_green_goal, min_red_goal) = (94, 147, 77)\n",
    "    (max_blue_goal, max_green_goal, max_red_goal) = (255, 255, 106)\n",
    "    \n",
    "    # Processing the obstacles to find the vertices and edges\n",
    "    processed_obstacles = process_image(image, min_blue_obst, min_green_obst, min_red_obst, max_blue_obst, max_green_obst, max_red_obst)  \n",
    "    (obstacle_contours, _) = cv2.findContours(processed_obstacles, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    obstacle_vertices, obstacle_edges, num_obstacles = process_obstacles(obstacle_contours)\n",
    "    \n",
    "    # Display the processed grayscale mask using matplotlib\n",
    "    plt.imshow(processed_obstacles, cmap='gray')\n",
    "    plt.title(\"goal\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Processing the goal destination to find the center\n",
    "    processed_goal = process_image(image, min_blue_goal, min_green_goal, min_red_goal, max_blue_goal, max_green_goal, max_red_goal)  \n",
    "    (goal_contours, _) = cv2.findContours(processed_goal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    goal_center = process_goal(goal_contours)\n",
    "    \n",
    "    # Display the processed grayscale mask using matplotlib\n",
    "    plt.imshow(processed_goal, cmap='gray')\n",
    "    plt.title(\"goal\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return obstacle_vertices, obstacle_edges, num_obstacles, goal_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShortestPath Using pyvisgraph\n",
    "Using PyVisGraph library, which creates an visiblity graph and uses Dijkstras algorithm to find the shortest path\n",
    "Source: https://github.com/TaipanRex/pyvisgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS: vertices of obstacles, the robot position, goal position\n",
    "def getShortestPath(shape_vertices, Rob_pos, Goal_pos):\n",
    "    polygons = []\n",
    "    for shape in shape_vertices:\n",
    "        polygon = []\n",
    "        for point in shape:\n",
    "            polygon.append(vg.Point(point[0], point[1]))\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    graph = vg.VisGraph()\n",
    "    graph.build(polygons)\n",
    "\n",
    "    startPosition = vg.Point(Rob_pos[0],Rob_pos[1])\n",
    "    endPosition = vg.Point(Goal_pos[0], Goal_pos[1])\n",
    "\n",
    "    shortestPath = graph.shortest_path(startPosition, endPosition)\n",
    "    #print(shortestPath)\n",
    "    return shortestPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrawPathGraph\n",
    "Function visualizes the paths that is generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS: the shortestPath: a vector of vg.Point vertices\n",
    "# shape_vertices: the vertices of expanded shapes\n",
    "# pathImage: the image to draw the path\n",
    "def drawPathGraph(shape_vertices, shortestPath, pathImage):\n",
    "    # drawing points for each expanded vertex in the shape\n",
    "    for vertices in shape_vertices:\n",
    "        for i, vertex in enumerate(vertices):\n",
    "            x = vertex[0]\n",
    "            y = vertex[1]\n",
    "            cv2.circle(pathImage, (x, y), 5, (255, 0, 0), -1)  \n",
    "    \n",
    "    # creating a list of edges to store path into\n",
    "    edgelist = []\n",
    "    for i, node in enumerate(shortestPath[:-1]):\n",
    "        print(shortestPath[i])\n",
    "        edgelist.append((shortestPath[i], shortestPath[i + 1]))\n",
    "        \n",
    "    color = (0, 255, 255)\n",
    "    thickness = 3\n",
    "    for i, edge in enumerate(edgelist):\n",
    "        #print(int(edgelist[i][0].x), int(edgelist[i][0].y))\n",
    "        #transformedEdge = cv2.perspectiveTransform(int(edgelist[i][0].x), int(edgelist[i][0].y))\n",
    "        cv2.line(pathImage, (int(edgelist[i][0].x), int(edgelist[i][0].y)), (int(edgelist[i][1].x), int(edgelist[i][1].y)), color, thickness)\n",
    "        \n",
    "    # drawing start and goal positions as circles\n",
    "    goal_location = shortestPath[-1]\n",
    "    robot_location = shortestPath[0]\n",
    "    cv2.circle(pathImage, (int(goal_location.x), int(goal_location.y)), 15, (255, 0, 0), 1)\n",
    "    cv2.circle(pathImage, (int(robot_location.x), int(robot_location.y)), 15, (0, 0, 255), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shortest_path(triangle_vertices, triangle_edges, goal_center, robot_center):\n",
    "\n",
    "    # Findinf the adjancy matrix used in the A* global shortest path\n",
    "    G = create_graph(triangle_vertices, triangle_edges, goal_center[0], goal_center[1], robot_center[0], robot_center[1])\n",
    "\n",
    "    euclidean_distances = calculate_euclidean_distances(G)\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, node_size=600, font_weight='bold')\n",
    "    # Draw the highlighted vertex of position of robot and goal position  in a different color\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[(goal_center[0], goal_center[1])], node_size=600, node_color='red')\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[(robot_center[0], robot_center[1])], node_size=600, node_color='green')\n",
    "\n",
    "    plt.title('Adjacency Graph with highlighted final destination in red and position of robot in green')\n",
    "    plt.show()\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw Path on camera feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_path_on_camera(cameraImage, shortestPath, obstacle_vertices, robot_center, robot_angle):\n",
    "    \n",
    "    # drawing the expanded_vertices\n",
    "    for vertices in obstacle_vertices:\n",
    "        for i, vertex in enumerate(vertices):\n",
    "            x = vertex[0]\n",
    "            y = vertex[1]\n",
    "            cv2.circle(new_perspective_image, (x, y), 10, (255, 0, 0), -1) \n",
    "    \n",
    "    # creating a list of edges from the list of vertices\n",
    "    edgelist = []\n",
    "    for i, node in enumerate(shortestPath[:-1]):\n",
    "        edgelist.append((shortestPath[i], shortestPath[i + 1]))\n",
    "        \n",
    "    color = (0, 255, 255)\n",
    "    thickness = 3\n",
    "    \n",
    "    # drawing the optimal path using edges\n",
    "    for i, edge in enumerate(edgelist):\n",
    "        #print(int(edgelist[i][0].x), int(edgelist[i][0].y))\n",
    "        # if the robot is already on the path of an edge, draw from that point\n",
    "        cv2.line(cameraImage, (int(edgelist[i][0].x), int(edgelist[i][0].y)), (int(edgelist[i][1].x), int(edgelist[i][1].y)), color, thickness)\n",
    "        \n",
    "    # drawing start and goal positions\n",
    "    goal_location = shortestPath[-1]\n",
    "    cv2.circle(cameraImage, (int(goal_location.x), int(goal_location.y)), 1, (255, 0, 0), -1)\n",
    "    cv2.circle(cameraImage, (int(robot_center[0]), int(robot_center[1])), 5, (0, 0, 255), -1)\n",
    "    \n",
    "    # draw a directional arrow of robot orientation\n",
    "    length = 50\n",
    "    endpoint_x = int(robot_center[0] + length * np.cos(np.radians(robot_angle)))\n",
    "    endpoint_y = int(robot_center[1] - length * np.sin(np.radians(robot_angle)))\n",
    "    cv2.arrowedLine(cameraImage, (int(robot_center[0]), int(robot_center[1])), (endpoint_x, endpoint_y), (255, 255, 0), 2)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking of QR code position and angle in the new perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# window_name = 'Tracking QR Code'\n",
    "# camera_id = 1 # Try changing with one of these (0,1,2..) because it depends how amny cameras are connected to your PC\n",
    "\n",
    "# video_stream = cv2.VideoCapture(camera_id)\n",
    "# time.sleep(1)\n",
    "# QR_detector = cv2.QRCodeDetector()\n",
    "\n",
    "# angle_window = []  # List to store angles over a window of frames\n",
    "# window_size = 5  # calculate the mean over that number of frames\n",
    "\n",
    "# transformation_matrix_found = False\n",
    "# transformation_matrix = None\n",
    "# background_found = False\n",
    "\n",
    "# num_obstacles = 0\n",
    "# # varible is true when first time running algorithm or when kidnapped\n",
    "# recalculate_global = True\n",
    "\n",
    "# # Check if the webcam is opened correctly\n",
    "# if not video_stream.isOpened():\n",
    "#     raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# while True:\n",
    "#     # Capture a frame from the video stream\n",
    "#     #print(\"here\")\n",
    "#     image_detected, image = video_stream.read()\n",
    "#     #find_thresh(image)\n",
    "#     if image_detected:\n",
    "#         image_initial = image.copy()\n",
    "#         height, width, channels = image.shape\n",
    "#         # Apply the new percpective on the frame\n",
    "#         if not transformation_matrix_found:\n",
    "#             transformation_matrix = perspective_transformation(image_initial)\n",
    "#             transformation_matrix_found = True\n",
    "#         if transformation_matrix_found:\n",
    "#             new_perspective_image = cv2.warpPerspective(image, transformation_matrix, (width, height))\n",
    "#         else:\n",
    "#             new_perspective_image = image\n",
    "#         #cv2.imshow(window_name, new_perspective_image)\n",
    "#         if not background_found or num_obstacles != 2:\n",
    "# #             plt.imshow(new_perspective_image)\n",
    "# #             plt.title(\"new perspective\")\n",
    "# #             plt.show()\n",
    "#             triangle_vertices, triangle_edges, goal_center = process_background(new_perspective_image)\n",
    "#             num_obstacles = len(triangle_vertices)\n",
    "#             print('vertices',triangle_vertices)\n",
    "#             print('edges',triangle_edges)\n",
    "#             print('goal',goal_center)\n",
    "#             background_found = True\n",
    "#             print('background found', background_found)\n",
    "#             continue\n",
    "       \n",
    "\n",
    "#         if background_found:    \n",
    "#             # Detect QR codes in the captured frame\n",
    "#             QR_detected, points, _ = QR_detector.detectAndDecode(image_initial)\n",
    "\n",
    "#             if QR_detected: \n",
    "#                 points = cv2.perspectiveTransform(points.reshape(-1, 1, 2), transformation_matrix)\n",
    "#                 points = points.reshape(1, 4, 2)\n",
    "\n",
    "#                 robot_angle, robot_center = orientation_angle(points)\n",
    "#                 #print('robot center',robot_center)\n",
    "#                 #print(f\"Individual Angle: {angle} degrees\")\n",
    "#                 angle_window.append(robot_angle)\n",
    "#                 if len(angle_window) == window_size:\n",
    "#                     mean_angle = calculate_mean_angle(angle_window)\n",
    "#                     #G = find_shortest_path(image, robot_center)\n",
    "#                     #print(f\"Mean Angle over {window_size} frames: {mean_angle} degrees\")\n",
    "#                     angle_window = []  # Reset the window for the next set of frames\n",
    "\n",
    "#                 #draw_vertex_circles(img_copy, points)  \n",
    "#                 color = (0, 255, 0)\n",
    "#                 new_perspective_image = cv2.polylines(new_perspective_image, [points.astype(int)], isClosed=True, color=color, thickness=8)\n",
    "                \n",
    "                \n",
    "#                 #print(points)\n",
    "#                 if recalculate_global:\n",
    "#                     shortest_path = getShortestPath(triangle_vertices, robot_center, goal_center)\n",
    "#                     recalculate_global = False\n",
    "                    \n",
    "#                     # visualize the optimal path\n",
    "#                     Pathimage = np.zeros([height,width,3],dtype=np.uint8)\n",
    "#                     Pathimage.fill(255)\n",
    "#                     drawPathGraph(triangle_vertices, shortest_path, Pathimage)\n",
    "#                     plt.imshow(Pathimage)\n",
    "#                     plt.title(\"Optimal Path\")\n",
    "#                     plt.show()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#                     #new_perspective_image = cv2.polylines(new_perspective_image, polyTriangle_vertices, isClosed=True, color=color, thickness=8)\n",
    "                \n",
    "#                 # Annotating the camera feed with the optimal path and vertices of obstacles\n",
    "#                 draw_path_on_camera(new_perspective_image, shortest_path, robot_center, robot_angle)    \n",
    "#                 # Display the modified image in the window\n",
    "#                 cv2.imshow(window_name, new_perspective_image)\n",
    "#                 #G = find_shortest_path(triangle_vertices, triangle_edges, goal_center, robot_center)\n",
    "\n",
    "#     # Exiting the loop when the 'Esc' key is pressed\n",
    "#     c = cv2.waitKey(1)\n",
    "#     if c == 27:\n",
    "#         break\n",
    "\n",
    "# video_stream.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STREAMLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrateHSV():\n",
    "    init_camera_QRdetector(CAMERA_ID)\n",
    "    image_detected, image = video_stream.read()\n",
    "    if image_detected:\n",
    "        find_thresh(image)\n",
    "    \n",
    "# function initializes the webcam            \n",
    "def init_camera_QRdetector(camera_id):\n",
    "    video_stream = cv2.VideoCapture(camera_id)\n",
    "    time.sleep(1)\n",
    "    QR_detector = cv2.QRCodeDetectorAruco()\n",
    "    if not video_stream.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    return video_stream, QR_detector\n",
    "    \n",
    "def kill_camera(video_stream):\n",
    "    video_stream.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m CAMERA_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      2\u001b[0m video_stream, _ \u001b[38;5;241m=\u001b[39m init_camera_QRdetector(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcalibrateHSV\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m kill_camera(video_stream)\n",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m, in \u001b[0;36mcalibrateHSV\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m image_detected, image \u001b[38;5;241m=\u001b[39m video_stream\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_detected:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mfind_thresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m, in \u001b[0;36mfind_thresh\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     41\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMask Image\u001b[39m\u001b[38;5;124m'\u001b[39m, mask)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# checking if q key is pressed to break out of loop\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CAMERA_ID = 1\n",
    "video_stream, _ = init_camera_QRdetector(1)\n",
    "calibrateHSV()\n",
    "kill_camera(video_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4220870132.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[25], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def change_shape_perspective(vertices, transformation_matrix):\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def change_shape_perspective(vertices, transformation_matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OBSTACLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_background(video_stream):\n",
    "    successInit = False  \n",
    "    transformation_matrix_found = False\n",
    "    transformation_matrix = None\n",
    "    background_found = False\n",
    "    num_obstacles = 0\n",
    "    \n",
    "    while not successInit:\n",
    "        image_detected, image = video_stream.read()\n",
    "        if image_detected:\n",
    "            image_initial = image.copy()\n",
    "            height, width, channels = image.shape\n",
    "            # this might be needed because the transformation matrix isnt always that good\n",
    "            transformation_matrix = perspective_transformation(image)\n",
    "            # Apply the new percpective on the frame\n",
    "            if not transformation_matrix_found:\n",
    "                transformation_matrix = perspective_transformation(image)\n",
    "                transformation_matrix_found = True\n",
    "            if transformation_matrix_found:\n",
    "                new_perspective_image = cv2.warpPerspective(image, transformation_matrix, (width, height))\n",
    "            else:\n",
    "                new_perspective_image = image\n",
    "            if not background_found:\n",
    "                #             plt.imshow(new_perspective_image)\n",
    "                #             plt.title(\"new perspective\")\n",
    "                #             plt.show()\n",
    "#                 obstacle_vertices, obstacle_edges, num_obstacles, goal_center = process_background(image_initial)\n",
    "#                 if (num_obstacles != 2):\n",
    "#                     continue\n",
    "#                 print(\"here\", obstacle_vertices)\n",
    "#                 obstacle_vertices = np.array(obstacle_vertices, dtype=float)\n",
    "#                 obstacle_vertices = cv2.perspectiveTransform(obstacle_vertices.reshape(-1, 1, 2), transformation_matrix)\n",
    "#                 obstacle_vertices = obstacle_vertices.reshape(1, 8, 2)\n",
    "#                 print(obstacle_vertices)\n",
    "                \n",
    "                \n",
    "                obstacle_vertices, obstacle_edges, num_obstacles, goal_center = process_background(new_perspective_image)\n",
    "                print('vertices', obstacle_vertices)\n",
    "                #print('edges', obstacle_edges)\n",
    "                print('goal', goal_center)\n",
    "                successInit = (num_obstacles == 2 and goal_center is not None) \n",
    "                print('background found', background_found)\n",
    "    \n",
    "    return obstacle_vertices, goal_center, transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robot_pos_angle(image, QR_detector, transformation_matrix):\n",
    "    robot_angle = None\n",
    "    robot_center = None\n",
    "    qr_vertices = None\n",
    "    window_size = 5\n",
    "    QR_detected, qr_vertices, _ = QR_detector.detectAndDecode(image)\n",
    "    if QR_detected: \n",
    "#         qr_vertices = cv2.perspectiveTransform(qr_vertices.reshape(-1, 1, 2), transformation_matrix)\n",
    "#         qr_vertices = qr_vertices.reshape(1, 4, 2)\n",
    "\n",
    "        robot_angle, robot_center = orientation_angle(qr_vertices)\n",
    "        print('robot center',robot_center)\n",
    "        #print(f\"Individual Angle: {angle} degrees\")\n",
    "        angle_window = []\n",
    "        angle_window.append(robot_angle)\n",
    "        if len(angle_window) == window_size:\n",
    "            mean_angle = calculate_mean_angle(angle_window)\n",
    "            #print(f\"Mean Angle over {window_size} frames: {mean_angle} degrees\")\n",
    "            angle_window = []  # Reset the window for the next set of frames\n",
    "    return robot_angle, robot_center, qr_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERA_ID = 1\n",
    "video_stream, QR_detector = init_camera_QRdetector(1)\n",
    "obstacle_vertices, goal_center, transformation_matrix = init_background(video_stream)\n",
    "recalculate_global = True\n",
    "last_robot_center = None\n",
    "windowName = \"Tracking\"\n",
    "while True:\n",
    "    image_detected, image = video_stream.read()\n",
    "    if image_detected:\n",
    "        image_initial = image.copy()\n",
    "        height, width, channels = image.shape\n",
    "        # this might be needed because the transformation matrix isnt always that good\n",
    "        new_perspective_image = cv2.warpPerspective(image, transformation_matrix, (width, height))\n",
    "        # returns the current position of robot\n",
    "        robot_angle, robot_center, qr_vertices = get_robot_pos_angle(new_perspective_image, QR_detector, transformation_matrix)\n",
    "        print(robot_center)\n",
    "        if robot_center is None:\n",
    "            continue\n",
    "        \n",
    "        if robot_center is not None:\n",
    "            last_robot_center = robot_center\n",
    "            new_perspective_image = cv2.polylines(new_perspective_image, [qr_vertices.astype(int)], isClosed=True, color=(255, 0, 0), thickness=8)\n",
    "            # Annotating the camera feed with the optimal path and vertices of obstacles\n",
    " \n",
    "        if recalculate_global:\n",
    "            shortest_path = getShortestPath(obstacle_vertices, last_robot_center, goal_center)\n",
    "            recalculate_global = False\n",
    "\n",
    "            # visualize the optimal path\n",
    "#             Pathimage = np.zeros([height,width,3],dtype=np.uint8)\n",
    "#             Pathimage.fill(255)\n",
    "#             drawPathGraph(obstacle_vertices, shortest_path, Pathimage)\n",
    "#             plt.imshow(Pathimage)\n",
    "#             plt.title(\"Optimal Path\")\n",
    "#             plt.show()\n",
    "\n",
    "        # draws the path on the camera feed\n",
    "        draw_path_on_camera(new_perspective_image, shortest_path, obstacle_vertices, last_robot_center, robot_angle) \n",
    "        \n",
    "        cv2.imshow(windowName, new_perspective_image)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "        \n",
    "kill_camera(video_stream)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n",
      "QR Code(s) detected\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_and_draw_qr_from_webcam():\n",
    "    # Open a connection to the webcam (assuming it's the first camera, use 0 for the default camera)\n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "    # Initialize the QR code detector\n",
    "    qr_detector = cv2.QRCodeDetectorAruco()\n",
    "\n",
    "    h = 0\n",
    "    w = 0\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret or frame is None:\n",
    "            print(\"Error: Couldn't read frame from the webcam.\")\n",
    "            continue\n",
    "            \n",
    "#         h, w, c = frame.shape\n",
    "#         #print(h, w)\n",
    "#         resized = cv2.resize(frame, (h*5, w*5))\n",
    "        \n",
    "        # Detect and decode QR codes in the frame\n",
    "        retval, points, _ = qr_detector.detectAndDecode(frame)\n",
    "\n",
    "        # Check if QR codes are detected\n",
    "        if retval:\n",
    "            print(\"QR Code(s) detected\")\n",
    "\n",
    "            # Draw rectangles around the QR codes\n",
    "            for point_set in points:\n",
    "                rect_pts = point_set.reshape(-1, 2).astype(np.int32)\n",
    "                cv2.polylines(frame, [rect_pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # Display the frame with rectangles around QR codes\n",
    "        cv2.imshow(\"QR Code Detection\", frame)\n",
    "\n",
    "        # Break the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function to start webcam stream and QR code detection\n",
    "detect_and_draw_qr_from_webcam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
